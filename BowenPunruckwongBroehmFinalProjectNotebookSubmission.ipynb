{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colin Bowen & Bhawramaett Punruckwong Broehm\n",
    "# 520.637: Foundations of Reinforcement Learning\n",
    "# Final Project Implementation Workflow\n",
    "# December 9, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Additional code is available at: https://github.com/colinpbowen/fantasy-RL. This notebook includes our model design and implementation. Additional files in the GitHub Repo are primarily for cleaning/preparing data for input to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Import Data, Functions, and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from xgoals import * #use getPts and getXPts functions\n",
    "import requests\n",
    "import pickle\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "def get_df(player):\n",
    "    '''access vaastav gw csvs'''\n",
    "    base_str = \"https://raw.githubusercontent.com/vaastav/Fantasy-Premier-League/master/data/2018-19/players/\"\n",
    "    url = base_str + player +\"/gw.csv\"\n",
    "    s = requests.get(url).text    \n",
    "    df = pd.read_csv(StringIO(s))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the optimal starting selection according to the Integer Program\n",
    "selection = pd.read_csv(\"https://raw.githubusercontent.com/colinpbowen/fantasy-RL/main/optimal_starting_team_1819.csv\",\n",
    "                       index_col=0) #the vector of 1s and 0s corresponding to the starting team\n",
    "player_initial_data = pd.read_csv('player_initial_values_1819.csv') #positional data\n",
    "full_names = player_initial_data['full_names'] #get player names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Selecting the top 50 percentile players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    max_selected = []\n",
    "    for player in full_names:\n",
    "        df = get_df(player)\n",
    "        try:\n",
    "            val = df['selected'].max()\n",
    "            max_selected.append(val)\n",
    "        except KeyError:\n",
    "            max_selected.append(0)\n",
    "\n",
    "    # This takes a minute to run, so pickle max_selected to save time for future runs\n",
    "    with open('max_selected.pkl', 'wb') as f:\n",
    "        pickle.dump(max_selected, f)\n",
    "# Load in max_selected:\n",
    "with open('max_selected.pkl', 'rb') as f:  \n",
    "    max_selected = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_selected = pd.Series(max_selected)\n",
    "idx = maximum_selected[maximum_selected > maximum_selected.mean()].index #152 players\n",
    "selection = selection.loc[idx] #reduces 500 initialization vector to 152\n",
    "selection = selection.to_numpy()[:,0]\n",
    "full_names = full_names.loc[idx]\n",
    "player_initial_data = player_initial_data.loc[idx]\n",
    "\n",
    "#pd.Series(idx).to_csv('idx.csv')\n",
    "#idx = pd.read_csv('idx.csv',index_col=0)\n",
    "#selection = selection.loc[idx] #reduces 500 initialization vector to 152\n",
    "#selection = selection.to_numpy()[:,0]\n",
    "#full_names = full_names.loc[idx]\n",
    "#player_initial_data = player_initial_data.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "player_initial_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Weekly Updates of Expected Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each epoch (pass through the data/episode/whatever):\n",
    "    \n",
    "    For each week:\n",
    "\n",
    "        1. Get integer program initialization for GW0.\n",
    "        2. Start the most selected team. \n",
    "        3. Observe GW1. Get points data for week 1\n",
    "        4. Update expected reward distribution.\n",
    "        5. Sample rewards for week 2 to get expected points for team and player pool.\n",
    "        6. Decide who to transfer for week 2.\n",
    "        7. Start whomever has the highest expected reward. Make captain the highest expected reward, regardless of position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to choose starters, make sure selection doesn't violate constraints (e.g., valid formation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_starters(curTeam, init=True):\n",
    "    '''\n",
    "    From cur_team, picks starters based upon xP criterion\n",
    "    If init=True, picks based upon most selected in GW1\n",
    "    Returns idx_vec: indexer into cur_team for computing the points return for the coming week.\n",
    "    '''\n",
    "    idx_vec = np.zeros((len(curTeam,)))\n",
    "    #if initializing, choose based upon most selected in each position\n",
    "    if init:\n",
    "        criterion = 'selected'\n",
    "    else:\n",
    "        criterion = 'xP'  # criterion = 'total_points'\n",
    "        #pick GK\n",
    "    out_pos = [2,3,4]\n",
    "    counts = [0,0,0] #position player counts\n",
    "    count_lim = [5,5,3]\n",
    "    GK = np.argmax(curTeam[criterion][curTeam.position == 1]) #returns idx of starting GK\n",
    "    GK = curTeam.full_name[curTeam.position == 1].iloc[GK] #get starting GK\n",
    "    idx_vec[curTeam.full_name==GK] = 1\n",
    "\n",
    "    #sort players based upon their point scoring\n",
    "    outfield = curTeam.position != 1\n",
    "    pos_sort = np.argsort(curTeam[criterion][outfield])[::-1]\n",
    "    for i in range(len(pos_sort)):\n",
    "\n",
    "        if sum(counts) == 10:\n",
    "            return idx_vec\n",
    "        \n",
    "        player = curTeam.full_name[outfield].iloc[pos_sort.iloc[i]]\n",
    "        #print(player)\n",
    "        player_pos = curTeam.position[curTeam.full_name==player].values[0]\n",
    "        #print(player_pos)\n",
    "        valid = check_valid(player_pos, counts)\n",
    "        #if a valid addition, then add player to team\n",
    "        \n",
    "        if valid:\n",
    "            #print('valid')\n",
    "            idxr = (out_pos==player_pos).astype(int)\n",
    "            #print(idxr)\n",
    "            counts += idxr\n",
    "            idx_vec[curTeam.full_name==player] = 1\n",
    "\n",
    "    return idx_vec.astype(bool)\n",
    "\n",
    "def check_valid(player_pos, counts):\n",
    "    '''check for valid addition to starters given formational constraints'''\n",
    "    out_pos = [2,3,4]\n",
    "    cur_count = sum(counts)\n",
    "    #valid formations are 3-4-3, 3-5-2, 4-4-2, 4-5-1, 4-3-3, 5-3-2, 5-4-1, 5-2-3\n",
    "    #teams are valid with at least three defenders, two midfielders, and one forward\n",
    "    if player_pos == 2:\n",
    "        #can't have a fifth defender if there are 5 mids\n",
    "        if counts[0] == 4 and counts[1] == 5:\n",
    "            return False\n",
    "        return True\n",
    "    if player_pos == 3:\n",
    "        if counts[1] == 4 and counts[0] == 5:\n",
    "            return False\n",
    "        return True \n",
    "    if player_pos == 4:\n",
    "        if counts[2]==2 and counts[1] == 5:\n",
    "            return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Budget, Current Team DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current budget\n",
    "def initializeTeam():\n",
    "    curTeam = pd.DataFrame()\n",
    "    cur_team = full_names[selection.astype(bool)]\n",
    "    #print(cur_team)\n",
    "    curTeam['full_name'] = cur_team\n",
    "    curTeam['position'] = player_initial_data.element_type[selection.astype(bool)]\n",
    "    curTeam['team'] = player_initial_data.team[selection.astype(bool)]\n",
    "    curTeam['value'] = player_initial_data.cost[selection.astype(bool)]\n",
    "    init_selected = []\n",
    "\n",
    "    budget = 1000\n",
    "    CURRENT_SCORE = 0\n",
    "    for player in cur_team:\n",
    "        df = get_df(player)\n",
    "        idx = df['round']==1\n",
    "        val = df.value[idx].values[0]\n",
    "        selec = df.selected[idx].values[0]\n",
    "        init_selected.append(selec)\n",
    "        budget -= val\n",
    "    curTeam['selected'] = init_selected\n",
    "    getCaptain = [1 if curTeam['selected'].iloc[x] == curTeam.selected.max() else 0 for x in range(len(curTeam))]\n",
    "    curTeam['isCaptain'] = getCaptain\n",
    "    curTeam['isStarting'] = pick_starters(curTeam, init=True)\n",
    "\n",
    "    return curTeam, budget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "curTeam, budget = initializeTeam()\n",
    "print(\"Budget remaining: {}\".format(budget))\n",
    "curTeam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "#### These functions help us update distributions for each player statistic so that we can get an expectation of points\n",
    "#### They also help us determine which players should be transferred and let us update our squad each week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice as ch\n",
    "from statistics import median\n",
    "\n",
    "# Set the number of times we want to run np.random.choice:\n",
    "numChoices = 500 # INPUT A VALUE HERE\n",
    "\n",
    "def getXpts(df, week, position):\n",
    "    \"\"\"\n",
    "    Generate simulation outcome for a particular week based upon prior distributions.\n",
    "    \"\"\"\n",
    "    xG = xGprobs(df,week)\n",
    "    xA = xAprobs(df, week)\n",
    "    xB = xBprobs(df, week)\n",
    "    xCS = xCSprobs(df, week)\n",
    "    xGC = xGCprobs(df, week)\n",
    "    xM = xMprobs(df, week)\n",
    "    xOG = xOGprobs(df, week)\n",
    "    xPC = xPCprobs(df, week)\n",
    "    xPM = xPMprobs(df, week)\n",
    "    xPS = xPSprobs(df, week)\n",
    "    xRC = xRCprobs(df, week)\n",
    "    xS = xSprobs(df, week)\n",
    "    xYC = xYCprobs(df, week)\n",
    "\n",
    "    G = median(ch(a=np.arange(len(xG)), p=xG, size=numChoices))\n",
    "    A = median(ch(a=np.arange(len(xA)),p=xA, size=numChoices))\n",
    "    B = median(ch(a=np.arange(len(xB)),p=xB, size=numChoices))\n",
    "    CS = median(ch(a=np.arange(len(xCS)),p=xCS, size=numChoices))\n",
    "    GC = median(ch(a=np.arange(len(xGC)),p=xGC, size=numChoices))\n",
    "    M = median(ch(a=np.arange(len(xM)),p=xM, size=numChoices))\n",
    "    OG = median(ch(a=np.arange(len(xOG)),p=xOG, size=numChoices))\n",
    "    PC = median(ch(a=np.arange(len(xPC)),p=xPC, size=numChoices))\n",
    "    PM = median(ch(a=np.arange(len(xPM)),p=xPM, size=numChoices))\n",
    "    PS = median(ch(a=np.arange(len(xPS)),p=xPS, size=numChoices))\n",
    "    RC = median(ch(a=np.arange(len(xRC)), p=xRC, size=numChoices))\n",
    "    S = median(ch(a=np.arange(len(xS)),p=xS, size=numChoices))\n",
    "    YC = median(ch(a=np.arange(len(xYC)),p=xYC, size=numChoices))\n",
    "    data = {'goals_scored':G, 'assists': A,'bonus': B,'clean_sheets': CS,'goals_conceded': GC,\n",
    "            'minutes': M,'own_goals': OG,'penalties_conceded': PC,'penalties_missed': PM,\n",
    "            'penalties_saved': PS, 'red_cards': RC, 'saves' : S,'yellow_cards' : YC}\n",
    "    return getPts(data, position)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def xGprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected goals for a player. \n",
    "    Args: df (player's gw DataFrame) ,week (int)\n",
    "    \"\"\"\n",
    "    gs = df['goals_scored'][0:week+1].value_counts().sort_index()\n",
    "    if gs.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0, 0, 0, 0]  # Some prior we can change later. This assume p(0 Goals) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(4+1))\n",
    "        stats = idx.join(gs).fillna(0)\n",
    "        return [stats.goals_scored[x]/stats.goals_scored.sum() for x in range(len(stats))]\n",
    "\n",
    "# ## Expected Assists\n",
    "def xAprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected assists for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    ass = df['assists'][0:week+1].value_counts().sort_index()\n",
    "    if ass.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0, 0, 0]  # Some prior we can change later. This assume p(0 assists) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(3+1))\n",
    "        stats = idx.join(ass).fillna(0)\n",
    "        return [stats.assists[x]/stats.assists.sum() for x in range(len(stats))]\n",
    "\n",
    "# ## Expected Bonus\n",
    "def xBprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected bonus for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    bon = df['bonus'][0:week+1].value_counts().sort_index()\n",
    "    if bon.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0, 0, 0]  # Some prior we can change later. This assume p(0 bonus) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(3+1))\n",
    "        stats = idx.join(bon).fillna(0)\n",
    "        return [stats.bonus[x]/stats.bonus.sum() for x in range(len(stats))]\n",
    "\n",
    "# ## Expected Clean Sheets\n",
    "\n",
    "def xCSprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected clean sheets for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    cs = df['clean_sheets'][0:week+1].value_counts().sort_index()\n",
    "    if cs.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(1+1))\n",
    "        stats = idx.join(cs).fillna(0)\n",
    "        return [stats.clean_sheets[x]/stats.clean_sheets.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "# ## Expected Goals Conceded\n",
    "def xGCprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected goals conceded for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    gc = df['goals_conceded'][0:week+1].value_counts().sort_index()\n",
    "    if gc.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0., 0., 0., 0., 0., 0., 0., 0., 0.]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(9+1))\n",
    "        stats = idx.join(gc).fillna(0)\n",
    "        return [stats.goals_conceded[x]/stats.goals_conceded.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "# ## Expected Minutes\n",
    "\n",
    "\n",
    "def xMprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected minutes for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    p = np.zeros(91)\n",
    "    p[0] = 1.0\n",
    "    M = df['minutes'][0:week+1].value_counts().sort_index()\n",
    "    if M.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return p  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(90+1))\n",
    "        stats = idx.join(M).fillna(0)\n",
    "        return [stats.minutes[x]/stats.minutes.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "# ## Expected Own Goals\n",
    "\n",
    "\n",
    "def xOGprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected own goals for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    og = df['own_goals'][0:week+1].value_counts().sort_index()\n",
    "    if og.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(1+1))\n",
    "        stats = idx.join(og).fillna(0)\n",
    "        return [stats.own_goals[x]/stats.own_goals.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "\n",
    "# ## Expected Penalties Conceded\n",
    "\n",
    "\n",
    "def xPCprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected penalties conceded for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pc = df['penalties_conceded'][0:week+1].value_counts().sort_index()\n",
    "    except KeyError: # this column is missing in 2019/2020 data for some players\n",
    "        return [1,0,0]\n",
    "    if pc.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0, 0]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(2+1))\n",
    "        stats = idx.join(pc).fillna(0)\n",
    "        return [stats.penalties_conceded[x]/stats.penalties_conceded.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "# ## Expected Penalties Missed\n",
    "\n",
    "def xPMprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected penalties missed for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    pm = df['penalties_missed'][0:week+1].value_counts().sort_index()\n",
    "    if pm.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(1+1))\n",
    "        stats = idx.join(pm).fillna(0)\n",
    "        return [stats.penalties_missed[x]/stats.penalties_missed.sum() for x in range(len(stats))]\n",
    "\n",
    "# ## Expected Penalties Saved\n",
    "\n",
    "def xPSprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected penalties saved for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    ps = df['penalties_saved'][0:week+1].value_counts().sort_index()\n",
    "    if ps.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0, 0]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(2+1))\n",
    "        stats = idx.join(ps).fillna(0)\n",
    "        return [stats.penalties_saved[x]/stats.penalties_saved.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Expected Red Cards\n",
    "\n",
    "def xRCprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected red cards for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    rc = df['red_cards'][0:week+1].value_counts().sort_index()\n",
    "    if rc.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(1+1))\n",
    "        stats = idx.join(rc).fillna(0)\n",
    "        return [stats.red_cards[x]/stats.red_cards.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "\n",
    "# ## Expected Saves\n",
    "\n",
    "def xSprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected saves for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    p = np.zeros(15)\n",
    "    p[0] = 1.0\n",
    "    s = df['saves'][0:week+1].value_counts().sort_index()\n",
    "    if s.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return p  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(14+1))\n",
    "        stats = idx.join(s).fillna(0)\n",
    "        return [stats.saves[x]/stats.saves.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "# ## Expected Yellow Cards\n",
    "\n",
    "\n",
    "def xYCprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected yellow cards for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    yc = df['yellow_cards'][0:week+1].value_counts().sort_index()\n",
    "    if yc.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(1+1))\n",
    "        stats = idx.join(yc).fillna(0)\n",
    "        return [stats.yellow_cards[x]/stats.yellow_cards.sum() for x in range(len(stats))]\n",
    "        \n",
    "#generate realization\n",
    "#no need to use this for accessing historical data because it's already stored? \n",
    "#need to add penalties conceded\n",
    "def getPts(x, position):\n",
    "    \"\"\" \n",
    "    Use with getXpts for simulating reward.\n",
    "    \"\"\"\n",
    "    pts = 0\n",
    "    pts += x['bonus']\n",
    "    if ((x['minutes'] > 0) & (x['minutes'] < 60)):\n",
    "        pts += 1\n",
    "    elif (x['minutes'] >=60):\n",
    "        pts += 2\n",
    "    if position == 1.0:  # Goalie\n",
    "        pts += 6*x['goals_scored'] + x['assists']*3 + 4*x['clean_sheets'] + int(x['saves']//3) + 5*x['penalties_saved'] - x['goals_conceded']//2 - x['yellow_cards'] - 3*x['red_cards'] - 2*x['own_goals']   \n",
    "    elif position == 2.0:  # Defender\n",
    "        pts += 6*x['goals_scored'] + x['assists']*3 + 4*x['clean_sheets'] - 2*x['penalties_missed']  - x['goals_conceded']//2 - x['yellow_cards'] - 3*x['red_cards'] - 2*x['own_goals']\n",
    "    elif position == 3.0: # Mid\n",
    "        pts += 5*x['goals_scored'] + x['assists']*3 + 1*x['clean_sheets'] - 2*x['penalties_missed'] - x['yellow_cards'] - 3*x['red_cards'] - 2*x['own_goals']\n",
    "    else:  # Striker\n",
    "        pts += 4*x['goals_scored'] + x['assists']*3 - 2*x['penalties_missed'] - x['yellow_cards'] - 3*x['red_cards'] - 2*x['own_goals']\n",
    "    return pts\n",
    "    \n",
    "def ilocfromloc(player):\n",
    "    \"\"\"\n",
    "    Use this function to get the relative position of a player in the full_names series.\n",
    "    Helpful because other objects such as xPs use the same ordering as full_names.\n",
    "    \"\"\"\n",
    "    return full_names.index.get_loc(full_names[full_names==player].index[0])\n",
    "\n",
    "def getPlayerPool(team=curTeam):\n",
    "    player_pool = set(full_names) - set(team.full_name)\n",
    "    pp = player_initial_data.loc[player_initial_data[\"full_names\"].isin(player_pool)][[\"full_names\", \"element_type\", 'team']]\n",
    "    pp['xP'] = 0\n",
    "    return pp\n",
    "\n",
    "def updateXP(week,team=curTeam):\n",
    "    for player in team.full_name.values:\n",
    "        team.loc[team.full_name == player, 'xP'] = getXpts(get_df(player), week, team.loc[team.full_name == player]['position'].values[0])\n",
    "    return team\n",
    "\n",
    "def selectCaptain(team=curTeam):\n",
    "    \"\"\"\n",
    "    input your starting squad and this function picks the captain based on whose xP is highest\n",
    "    \"\"\"\n",
    "    getCaptain = [1 if team['xP'].iloc[x] == team.xP.max() else 0 for x in range(len(team))]\n",
    "    team['isCaptain'] = getCaptain\n",
    "    return team\n",
    "\n",
    "def getTopN(df, position, week,N, max_budget):\n",
    "    for player in df.full_names.loc[df.element_type==position]: \n",
    "        #print(player)\n",
    "        df.loc[df.full_names == player, \"xP\"] = getXpts(get_df(player),week,position)\n",
    "        #df.loc[df.full_names == player, \"value\"] = get_df(player)['value'][week]\n",
    "        try:\n",
    "            df.loc[df.full_names == player, \"value\"] = get_df(player).loc[get_df(player)['round'] == week]['value'].iloc[0]\n",
    "        except IndexError: # If this player wasn't available in the week, we'll be able to exclude them using the following\n",
    "            df.loc[df.full_names == player, \"value\"] = max_budget + 1 \n",
    "    temp = df.loc[df.element_type==position].sort_values(by='xP',ascending=False)\n",
    "    temp = temp[temp.value <= max_budget]\n",
    "    return temp[0:N].rename(columns={\"full_names\":'full_name', \"element_type\":\"position\"})\n",
    "\n",
    "def worstAtPos(pos, team=curTeam, bud=budget):\n",
    "    \"\"\"\n",
    "    Input your current team, the current budget, and the position to replace.\n",
    "    Returns the player who has the lowest expected points per value at the specified position and the available budget if\n",
    "    that player were replaced\n",
    "    \"\"\"\n",
    "    options = team.loc[team.position == pos] # players \n",
    "    worst = options.iloc[np.argmin([options.iloc[x]['xP']/options.iloc[x]['value'] for x in range(len(options))])]\n",
    "    return worst.full_name, bud+worst.value\n",
    "\n",
    "def swapPlayer(team, playerOut, choices, idx, curBudget):\n",
    "    \"\"\"\n",
    "    Given players to swap and the current budget\n",
    "    Swaps a the player from the current team with the player in the player pool and updates the budget\n",
    "    Should figure out a way to integrate this with epsilon-greedy policy\n",
    "    \"\"\"\n",
    "    newTeam = pd.concat([team.loc[team.full_name!=playerOut], choices.iloc[[idx]]])\n",
    "    newBudget = curBudget + team.loc[team.full_name==playerOut]['value'].values[0] - choices.iloc[[idx]]['value'].values[0]\n",
    "    return newTeam, newBudget   \n",
    "\n",
    "def updateSquad(action, week, team=curTeam, bud=budget, PAs=3):\n",
    "    if action == 0: # Do Nothing\n",
    "        return team, bud\n",
    "    elif action < PAs+1: # Replace GoalKeeper\n",
    "        worstPlayer, potential_budget = worstAtPos(pos=1, team=team, bud=bud)\n",
    "        topN = getTopN(df=pp, position=1, week=week, N=PAs, max_budget=potential_budget)\n",
    "        team, budget = swapPlayer(team=team, playerOut=worstPlayer, choices=topN, idx=action-1, curBudget=bud)\n",
    "        return team, budget\n",
    "    elif action < PAs+4: # Replace Defender\n",
    "        worstPlayer, potential_budget = worstAtPos(pos=2, team=team, bud=bud)\n",
    "        topN = getTopN(df=pp, position=2, week=week, N=PAs, max_budget=potential_budget)\n",
    "        team, budget = swapPlayer(team=team, playerOut=worstPlayer, choices=topN, idx=action-PAs-1, curBudget=bud)\n",
    "        return team, budget\n",
    "    elif action < PAs+7: # Replace Midfielder\n",
    "        worstPlayer, potential_budget = worstAtPos(pos=3, team=team, bud=bud)\n",
    "        topN = getTopN(df=pp, position=3, week=week, N=PAs, max_budget=potential_budget)\n",
    "        team, budget = swapPlayer(team=team, playerOut=worstPlayer, choices=topN, idx=action-2*PAs-1, curBudget=bud)\n",
    "        return team, budget\n",
    "    else: # Replace Forward\n",
    "        worstPlayer, potential_budget = worstAtPos(pos=4, team=team, bud=bud)\n",
    "        topN = getTopN(df=pp, position=4, week=week, N=PAs, max_budget=potential_budget)\n",
    "        team, budget = swapPlayer(team=team, playerOut=worstPlayer, choices=topN, idx=action-3*PAs-1, curBudget=bud)\n",
    "        return team, budget\n",
    "    return\n",
    "\n",
    "def get_weekly_pts_data(week):\n",
    "    pts = []\n",
    "    for player in list(curTeam.loc[curTeam.isStarting == 1].full_name):\n",
    "        df = get_df(player)\n",
    "        idx = df['round'] == week\n",
    "        try:\n",
    "            pt = df['total_points'][idx].iloc[0]\n",
    "        except IndexError:\n",
    "            pt = 2  # If our starter doesn't play this week, assume their replacement got 2 pts (played 60+ min)\n",
    "        if curTeam.loc[curTeam['full_name'] == player, 'isCaptain'].values[0] == 1:\n",
    "            pt *=2\n",
    "        pts.append(pt)\n",
    "    return sum(pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the state-action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Possible Actions:\n",
    "    - Do Nothing\n",
    "    - Replace GK with top 5 GKs\n",
    "    - Replace Def/Mid/Attack with top 10 at that position\n",
    "    - 10 + 10 + 10 + 5 + 1 = 36\n",
    "- Possible States:\n",
    "    - Budget value: 0, 1, ..., B\n",
    "\n",
    "**Need to encode each action with a number (0-35)**\n",
    "    - 0: Do Nothing\n",
    "    - 1-5: Swap Worst GK with top 1-5 GK\n",
    "    - 6-15: Swap Worst Def with top 1-10 Def\n",
    "    - 16-25: Swap Worst Mid with top 1-10 Mid\n",
    "    - 26-35: Swap Worst Forward with top 1-10 Forward\n",
    " \n",
    "**Ultimately reduced to top 3 at each position because exploration was insuffucient in 100 episodes when |A(s)| = 36**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "########## SARSA ##########\n",
    "###########################\n",
    "\n",
    "# Initialize Algorithm Parameters\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "gamma = 1\n",
    "# gamma = 0.5 # Suggested by Mathews \n",
    "ssize = 2000 # number of possible states ==> for each possible budget and team configuration\n",
    "asize = 36 # number of possible actions in each state\n",
    "Qsarsa = np.zeros((ssize,asize))\n",
    "numEpisodes = 100\n",
    "weeks = list(range(2,39))\n",
    "sarsaRewards = np.zeros((numEpisodes,))\n",
    "WEEKLY_SCORES = np.zeros((numEpisodes,1,38+1))\n",
    "# Algorithm Functions\n",
    "def get_action(state, Q, epsilon):\n",
    "    Q_s = Q[state,:] # Get state-action rewards corresponding to the current state\n",
    "    if (np.random.uniform() <= epsilon):\n",
    "        action = np.random.choice(a=list(range(asize))) # Chose action randomily with pr 1-epsilon\n",
    "    else: # Choose greedy action with pr 1-epsilon\n",
    "        action = np.random.choice(np.flatnonzero(Q_s == Q_s.max())) \n",
    "    return action\n",
    "\n",
    "def SARSAupdate_Q(Q, S, Sprime, A, Aprime, alpha, R, gamma):\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Q[Sprime,Aprime] - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "def QLupdateQ(Q, S, Sprime, A, alpha, R, gamma):\n",
    "    Qmax = np.max(Q[Sprime, :])\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Qmax - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "ST = time.time()\n",
    "\n",
    "for i in range(numEpisodes):\n",
    "    episodeST = time.time()\n",
    "    print(\"starting episode {}\".format(i))\n",
    "    curTeam, budget = initializeTeam() # initialize the team and budget (state)\n",
    "    current_state = budget  # initialize S\n",
    "    sarsaRewards[i] += get_weekly_pts_data(1)\n",
    "    WEEKLY_SCORES[i,0,0] += get_weekly_pts_data(1)\n",
    "    for curWeek in weeks: # For each step in the episode\n",
    "        current_action = get_action(state=current_state, Q=Qsarsa, epsilon=epsilon) # Choose A from S using policy derived from Q\n",
    "        pp = getPlayerPool(team=curTeam) # Start by getting the available player pool\n",
    "        curTeam = updateXP(week=curWeek) # Get expectations for your current team's points\n",
    "        curTeam, next_state = updateSquad(current_action, week=curWeek, team=curTeam, bud=current_state)  # Update squad based on action, transition to new state (budget)\n",
    "        curTeam['isStarting'] = pick_starters(curTeam, init=False) # Choose your starters once you've updated the squad\n",
    "        curTeam = selectCaptain()  # Choose the captain once you have the starters\n",
    "        sarsaRewards[i] += get_weekly_pts_data(curWeek) # Take Action a, observe\n",
    "        WEEKLY_SCORES[i,0,curWeek] += get_weekly_pts_data(curWeek) # Realize GW results, update your Rewards\n",
    "        next_action = get_action(int(next_state), Qsarsa, epsilon)  # Choose A' from S' using policy derived from Q\n",
    "        Qsarsa = SARSAupdate_Q(Q=Qsarsa, S=int(current_state), Sprime=int(next_state), A=int(current_action), Aprime=int(next_action), alpha=alpha, R=WEEKLY_SCORES[i,0,curWeek], gamma=gamma)\n",
    "        current_state = int(next_state) # S <- S'\n",
    "        current_action = int(next_action) # A <- A'\n",
    "    episodeEND = time.time()\n",
    "    print(\"Episode {} took {:.2f} minutes to run. EPISODE PTS = {}\".format(i, (episodeST-episodeEND)/60, sarsaRewards[i]))\n",
    "    \n",
    "END = time.time()\n",
    "\n",
    "print(END-ST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(sarsaRewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Season Rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 episodes doesn't look like enough for sufficient training. Reducing the number of \"promising actions\" to top 3 at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "########## SARSA ##########\n",
    "###########################\n",
    "\n",
    "# Initialize Algorithm Parameters\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "gamma = 1\n",
    "# gamma = 0.5 # Suggested by Mathews \n",
    "ssize = 2000 # number of possible states ==> for each possible budget and team configuration\n",
    "asize = 13 # number of possible actions in each state: 13 now because do nothing + 3 PAs for each of 4 positions\n",
    "Qsarsa = np.zeros((ssize,asize))\n",
    "numEpisodes = 100\n",
    "weeks = list(range(2,39))\n",
    "sarsaRewards = np.zeros((numEpisodes,))\n",
    "WEEKLY_SCORES = np.zeros((numEpisodes,1,38+1))\n",
    "# Algorithm Functions\n",
    "def get_action(state, Q, epsilon):\n",
    "    Q_s = Q[state,:] # Get state-action rewards corresponding to the current state\n",
    "    if (np.random.uniform() <= epsilon):\n",
    "        action = np.random.choice(a=list(range(asize))) # Chose action randomily with pr 1-epsilon\n",
    "    else: # Choose greedy action with pr 1-epsilon\n",
    "        action = np.random.choice(np.flatnonzero(Q_s == Q_s.max())) \n",
    "    return action\n",
    "\n",
    "def SARSAupdate_Q(Q, S, Sprime, A, Aprime, alpha, R, gamma):\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Q[Sprime,Aprime] - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "def QLupdateQ(Q, S, Sprime, A, alpha, R, gamma):\n",
    "    Qmax = np.max(Q[Sprime, :])\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Qmax - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "ST = time.time()\n",
    "\n",
    "for i in range(numEpisodes):\n",
    "    episodeST = time.time()\n",
    "    print(\"starting episode {}\".format(i))\n",
    "    curTeam, budget = initializeTeam() # initialize the team and budget (state)\n",
    "    current_state = budget  # initialize S\n",
    "    sarsaRewards[i] += get_weekly_pts_data(1)\n",
    "    WEEKLY_SCORES[i,0,0] += get_weekly_pts_data(1)\n",
    "    for curWeek in weeks: # For each step in the episode\n",
    "        current_action = get_action(state=current_state, Q=Qsarsa, epsilon=epsilon) # Choose A from S using policy derived from Q\n",
    "        pp = getPlayerPool(team=curTeam) # Start by getting the available player pool\n",
    "        curTeam = updateXP(week=curWeek) # Get expectations for your current team's points\n",
    "        curTeam, next_state = updateSquad(current_action, week=curWeek, team=curTeam, bud=current_state)  # Update squad based on action, transition to new state (budget)\n",
    "        curTeam['isStarting'] = pick_starters(curTeam, init=False) # Choose your starters once you've updated the squad\n",
    "        curTeam = selectCaptain()  # Choose the captain once you have the starters\n",
    "        sarsaRewards[i] += get_weekly_pts_data(curWeek) # Take Action a, observe\n",
    "        WEEKLY_SCORES[i,0,curWeek] += get_weekly_pts_data(curWeek) # Realize GW results, update your Rewards\n",
    "        next_action = get_action(int(next_state), Qsarsa, epsilon)  # Choose A' from S' using policy derived from Q\n",
    "        Qsarsa = SARSAupdate_Q(Q=Qsarsa, S=int(current_state), Sprime=int(next_state), A=int(current_action), Aprime=int(next_action), alpha=alpha, R=WEEKLY_SCORES[i,0,curWeek], gamma=gamma)\n",
    "        current_state = int(next_state) # S <- S'\n",
    "        current_action = int(next_action) # A <- A'\n",
    "    episodeEND = time.time()\n",
    "    print(\"Episode {} took {:.2f} minutes to run. EPISODE PTS = {}\".format(i, (episodeEND-episodeST)/60, sarsaRewards[i]))\n",
    "    \n",
    "END = time.time()\n",
    "\n",
    "print(END-ST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PICKLE THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SARSA1.pkl', 'wb') as f:  \n",
    "    pickle.dump([Qsarsa, sarsaRewards, WEEKLY_SCORES], f)  # SAVE THE Trained Q function, Episodic Rewards, and Weekly Rewards\n",
    "\n",
    "with open('SARSA1.pkl', 'rb') as f:  \n",
    "    obj0, obj1, obj2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd.DataFrame(sarsaRewards).rolling(window=10).mean())\n",
    "pd.DataFrame(sarsaRewards).rolling(window=20).mean().iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(sarsaRewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Season Rewards\")\n",
    "plt.title(\"SARSA, γ=0.5\")\n",
    "plt.savefig(\"SARSA_Training_gamma5.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "########## Q-Learning ##########\n",
    "################################\n",
    "\n",
    "# Initialize Algorithm Parameters\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "gamma = 1\n",
    "# gamma = 0.5 # Suggested by Mathews \n",
    "ssize = 2000 # number of possible states ==> set to something sufficiently large so that we don't get IndexError\n",
    "asize = 13 # number of possible actions in each state: 13 now because do nothing + 3 PAs for each of 4 positions\n",
    "Qlearning = np.zeros((ssize,asize))\n",
    "numEpisodes = 100\n",
    "weeks = list(range(2,39))\n",
    "QLRewards = np.zeros((numEpisodes,))\n",
    "QL_WEEKLY_SCORES = np.zeros((numEpisodes,1,38+1))\n",
    "# Algorithm Functions\n",
    "def get_action(state, Q, epsilon):\n",
    "    Q_s = Q[state,:] # Get state-action rewards corresponding to the current state\n",
    "    if (np.random.uniform() <= epsilon):\n",
    "        action = np.random.choice(a=list(range(asize))) # Chose action randomily with pr 1-epsilon\n",
    "    else: # Choose greedy action with pr 1-epsilon\n",
    "        action = np.random.choice(np.flatnonzero(Q_s == Q_s.max())) \n",
    "    return action\n",
    "\n",
    "def SARSAupdate_Q(Q, S, Sprime, A, Aprime, alpha, R, gamma):\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Q[Sprime,Aprime] - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "def QLupdateQ(Q, S, Sprime, A, alpha, R, gamma):\n",
    "    Qmax = np.max(Q[Sprime, :])\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Qmax - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "ST = time.time()\n",
    "\n",
    "for i in range(numEpisodes):\n",
    "    episodeST = time.time()\n",
    "    print(\"starting episode {}\".format(i))\n",
    "    curTeam, budget = initializeTeam() # initialize the team and budget (state)\n",
    "    current_state = budget  # initialize S\n",
    "    QLRewards[i] += get_weekly_pts_data(1)\n",
    "    QL_WEEKLY_SCORES[i,0,0] += get_weekly_pts_data(1)\n",
    "    for curWeek in weeks: # For each step in the episode\n",
    "        current_action = get_action(state=current_state, Q=Qlearning, epsilon=epsilon) # Choose A from S using policy derived from Q\n",
    "        pp = getPlayerPool(team=curTeam) # Start by getting the available player pool\n",
    "        curTeam = updateXP(week=curWeek) # Get expectations for your current team's points\n",
    "        curTeam, next_state = updateSquad(current_action, week=curWeek, team=curTeam, bud=current_state)  # Update squad based on action, transition to new state (budget)\n",
    "        curTeam['isStarting'] = pick_starters(curTeam, init=False) # Choose your starters once you've updated the squad\n",
    "        curTeam = selectCaptain()  # Choose the captain once you have the starters\n",
    "        QLRewards[i] += get_weekly_pts_data(curWeek) # Take Action a, observe\n",
    "        QL_WEEKLY_SCORES[i,0,curWeek] += get_weekly_pts_data(curWeek) # Realize GW results, update your Rewards\n",
    "        Qlearning = QLupdateQ(Q=Qlearning, S=int(current_state), Sprime=int(next_state), A=int(current_action), alpha=alpha, R=QL_WEEKLY_SCORES[i,0,curWeek], gamma=gamma)\n",
    "        current_state = int(next_state) # S <- S'\n",
    "    episodeEND = time.time()\n",
    "    print(\"Episode {} took {:.2f} minutes to run. EPISODE PTS = {}\".format(i, (episodeEND-episodeST)/60, QLRewards[i]))\n",
    "    \n",
    "END = time.time()\n",
    "\n",
    "print(END-ST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(QLRewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Season Rewards\")\n",
    "plt.title(\"Q-Learning, γ=1\")\n",
    "plt.savefig(\"QL_Training_gamma1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let gamma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "########## Q-Learning ##########\n",
    "################################\n",
    "\n",
    "# Initialize Algorithm Parameters\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "# gamma = 1\n",
    "gamma = 0.5 # Suggested by Mathews \n",
    "ssize = 2000 # number of possible states ==> set to something sufficiently large so that we don't get IndexError\n",
    "asize = 13 # number of possible actions in each state: 13 now because do nothing + 3 PAs for each of 4 positions\n",
    "Qlearning = np.zeros((ssize,asize))\n",
    "numEpisodes = 100\n",
    "weeks = list(range(2,39))\n",
    "QLRewards = np.zeros((numEpisodes,))\n",
    "QL_WEEKLY_SCORES = np.zeros((numEpisodes,1,38+1))\n",
    "# Algorithm Functions\n",
    "def get_action(state, Q, epsilon):\n",
    "    Q_s = Q[state,:] # Get state-action rewards corresponding to the current state\n",
    "    if (np.random.uniform() <= epsilon):\n",
    "        action = np.random.choice(a=list(range(asize))) # Chose action randomily with pr 1-epsilon\n",
    "    else: # Choose greedy action with pr 1-epsilon\n",
    "        action = np.random.choice(np.flatnonzero(Q_s == Q_s.max())) \n",
    "    return action\n",
    "\n",
    "def SARSAupdate_Q(Q, S, Sprime, A, Aprime, alpha, R, gamma):\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Q[Sprime,Aprime] - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "def QLupdateQ(Q, S, Sprime, A, alpha, R, gamma):\n",
    "    Qmax = np.max(Q[Sprime, :])\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Qmax - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "ST = time.time()\n",
    "\n",
    "for i in range(numEpisodes):\n",
    "    episodeST = time.time()\n",
    "    print(\"starting episode {}\".format(i))\n",
    "    curTeam, budget = initializeTeam() # initialize the team and budget (state)\n",
    "    current_state = budget  # initialize S\n",
    "    QLRewards[i] += get_weekly_pts_data(1)\n",
    "    QL_WEEKLY_SCORES[i,0,0] += get_weekly_pts_data(1)\n",
    "    for curWeek in weeks: # For each step in the episode\n",
    "        current_action = get_action(state=current_state, Q=Qlearning, epsilon=epsilon) # Choose A from S using policy derived from Q\n",
    "        pp = getPlayerPool(team=curTeam) # Start by getting the available player pool\n",
    "        curTeam = updateXP(week=curWeek) # Get expectations for your current team's points\n",
    "        curTeam, next_state = updateSquad(current_action, week=curWeek, team=curTeam, bud=current_state)  # Update squad based on action, transition to new state (budget)\n",
    "        curTeam['isStarting'] = pick_starters(curTeam, init=False) # Choose your starters once you've updated the squad\n",
    "        curTeam = selectCaptain()  # Choose the captain once you have the starters\n",
    "        QLRewards[i] += get_weekly_pts_data(curWeek) # Take Action a, observe\n",
    "        QL_WEEKLY_SCORES[i,0,curWeek] += get_weekly_pts_data(curWeek) # Realize GW results, update your Rewards\n",
    "        Qlearning = QLupdateQ(Q=Qlearning, S=int(current_state), Sprime=int(next_state), A=int(current_action), alpha=alpha, R=QL_WEEKLY_SCORES[i,0,curWeek], gamma=gamma)\n",
    "        current_state = int(next_state) # S <- S'\n",
    "    episodeEND = time.time()\n",
    "    print(\"Episode {} took {:.2f} minutes to run. EPISODE PTS = {}\".format(i, (episodeEND-episodeST)/60, QLRewards[i]))\n",
    "    \n",
    "END = time.time()\n",
    "\n",
    "print(\"It took {:.2f} minutes to train 100 episodes.\".format((END-ST)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(QLRewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Season Rewards\")\n",
    "plt.title(\"Q-Learning, γ=0.5\")\n",
    "plt.savefig(\"QL_Training_gamma5.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Actions Randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numSims = 100\n",
    "action = np.random.choice(a=list(range(13)))\n",
    "weeks = list(range(2,39))\n",
    "# weeks = list(range(2,27))\n",
    "startTime = time.time()\n",
    "CURRENT_SCORE = np.zeros((numSims,))\n",
    "WEEKLY_SCORE = []\n",
    "curTeam, budget = initializeTeam()\n",
    "WEEKLY_SCORE.append(get_weekly_pts_data(1))\n",
    "CURRENT_SCORE += get_weekly_pts_data(1)\n",
    "action_list = []  # Keep track of actions\n",
    "for i in range(numSims):\n",
    "    print(\"starting simulation {}\".format(i))\n",
    "    for curWeek in weeks:\n",
    "        curTeam, budget = initializeTeam()\n",
    "        action = np.random.choice(a=list(range(13)))\n",
    "        action_list.append(action)\n",
    "        pp = getPlayerPool(team=curTeam) # Start by getting the available player pool\n",
    "        curTeam = updateXP(week=curWeek) # Get expectations for your current team's points\n",
    "        curTeam, budget = updateSquad(action, week=curWeek, team=curTeam, bud=budget)  # Update squad based on action, state (budget)\n",
    "        curTeam['isStarting'] = pick_starters(curTeam, init=False) # Choose your starters once you've updated the squad\n",
    "        curTeam = selectCaptain()  # Choose the captain once you have the starters\n",
    "        WEEKLY_SCORE.append(get_weekly_pts_data(curWeek)) # Realize GW results, update your Rewards\n",
    "        CURRENT_SCORE[i] += get_weekly_pts_data(curWeek)  # Realize GW results, update your Rewards\n",
    "endTime = time.time()\n",
    "print(\"{:.2f} minutes\".format((endTime-startTime)/60))\n",
    "print(\"End of Season Score: {}\".format(CURRENT_SCORE[i]))\n",
    "\n",
    "with open('RandomAction.pkl', 'wb') as f:  \n",
    "    pickle.dump([CURRENT_SCORE, WEEKLY_SCORE], f)  # SAVE THE Trained Q function, Episodic Rewards, and Weekly Rewards\n",
    "\n",
    "with open('RandomAction.pkl', 'rb') as f:  \n",
    "    obj0, obj1 = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(CURRENT_SCORE)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Season Rewards\")\n",
    "plt.title(\"Random Action Selection\")\n",
    "plt.savefig(\"RandomActionSelection10Sims.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on 2019-2020 Season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the get_df function to pull data from 2019-2020 season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from xgoals import * #use getPts and getXPts functions\n",
    "import requests\n",
    "import pickle\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "def get_df(player):\n",
    "    '''access vaastav gw csvs'''\n",
    "    base_str = \"https://raw.githubusercontent.com/vaastav/Fantasy-Premier-League/master/data/2019-20/players/\"\n",
    "    url = base_str + player +\"/gw.csv\"\n",
    "    s = requests.get(url).text    \n",
    "    df = pd.read_csv(StringIO(s))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the optimal starting selection according to the Integer Program\n",
    "selection = pd.read_csv(\"https://raw.githubusercontent.com/colinpbowen/fantasy-RL/main/optimal_starting_team_1920.csv\",\n",
    "                       index_col=0) #the vector of 1s and 0s corresponding to the starting team\n",
    "player_initial_data = pd.read_csv('player_initial_values_1920.csv') #positional data\n",
    "full_names = player_initial_data['full_names'] #get player names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    max_selected = []\n",
    "    for player in full_names:\n",
    "        df = get_df(player)\n",
    "        try:\n",
    "            val = df['selected'].max()\n",
    "            max_selected.append(val)\n",
    "        except KeyError:\n",
    "            max_selected.append(0)\n",
    "\n",
    "    # This takes a minute to run, so pickle max_selected to save time for future runs\n",
    "    with open('max_selected_1920.pkl', 'wb') as f:\n",
    "        pickle.dump(max_selected, f)\n",
    "# Load in max_selected:\n",
    "with open('max_selected_1920.pkl', 'rb') as f:  \n",
    "    max_selected = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_selected = pd.Series(max_selected)\n",
    "idx = maximum_selected[maximum_selected > maximum_selected.mean()].index #152 players\n",
    "selection = selection.loc[idx] #reduces 500 initialization vector to 152\n",
    "selection = selection.to_numpy()[:,0]\n",
    "full_names = full_names.loc[idx]\n",
    "player_initial_data = player_initial_data.loc[idx]\n",
    "player_initial_data\n",
    "#pd.Series(idx).to_csv('idx.csv')\n",
    "#idx = pd.read_csv('idx.csv',index_col=0)\n",
    "#selection = selection.loc[idx] #reduces 500 initialization vector to 152\n",
    "#selection = selection.to_numpy()[:,0]\n",
    "#full_names = full_names.loc[idx]\n",
    "#player_initial_data = player_initial_data.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "curTeam, budget = initializeTeam()\n",
    "print(\"Budget remaining: {}\".format(budget))\n",
    "curTeam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Helper functions for 2019-2020\n",
    "- We run into a few issues due to differences in data across seasons. For example, some players are missing the \"Penalties Conceded\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice as ch\n",
    "from statistics import median\n",
    "\n",
    "# Set the number of times we want to run np.random.choice:\n",
    "numChoices = 500 # INPUT A VALUE HERE\n",
    "\n",
    "def getXpts(df, week, position):\n",
    "    \"\"\"\n",
    "    Generate simulation outcome for a particular week based upon prior distributions.\n",
    "    \"\"\"\n",
    "    xG = xGprobs(df,week)\n",
    "    xA = xAprobs(df, week)\n",
    "    xB = xBprobs(df, week)\n",
    "    xCS = xCSprobs(df, week)\n",
    "    xGC = xGCprobs(df, week)\n",
    "    xM = xMprobs(df, week)\n",
    "    xOG = xOGprobs(df, week)\n",
    "    xPC = xPCprobs(df, week)\n",
    "    xPM = xPMprobs(df, week)\n",
    "    xPS = xPSprobs(df, week)\n",
    "    xRC = xRCprobs(df, week)\n",
    "    xS = xSprobs(df, week)\n",
    "    xYC = xYCprobs(df, week)\n",
    "\n",
    "    G = median(ch(a=np.arange(len(xG)), p=xG, size=numChoices))\n",
    "    A = median(ch(a=np.arange(len(xA)),p=xA, size=numChoices))\n",
    "    B = median(ch(a=np.arange(len(xB)),p=xB, size=numChoices))\n",
    "    CS = median(ch(a=np.arange(len(xCS)),p=xCS, size=numChoices))\n",
    "    GC = median(ch(a=np.arange(len(xGC)),p=xGC, size=numChoices))\n",
    "    M = median(ch(a=np.arange(len(xM)),p=xM, size=numChoices))\n",
    "    OG = median(ch(a=np.arange(len(xOG)),p=xOG, size=numChoices))\n",
    "    PC = median(ch(a=np.arange(len(xPC)),p=xPC, size=numChoices))\n",
    "    PM = median(ch(a=np.arange(len(xPM)),p=xPM, size=numChoices))\n",
    "    PS = median(ch(a=np.arange(len(xPS)),p=xPS, size=numChoices))\n",
    "    RC = median(ch(a=np.arange(len(xRC)), p=xRC, size=numChoices))\n",
    "    S = median(ch(a=np.arange(len(xS)),p=xS, size=numChoices))\n",
    "    YC = median(ch(a=np.arange(len(xYC)),p=xYC, size=numChoices))\n",
    "    data = {'goals_scored':G, 'assists': A,'bonus': B,'clean_sheets': CS,'goals_conceded': GC,\n",
    "            'minutes': M,'own_goals': OG,'penalties_conceded': PC,'penalties_missed': PM,\n",
    "            'penalties_saved': PS, 'red_cards': RC, 'saves' : S,'yellow_cards' : YC}\n",
    "    return getPts(data, position)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def xGprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected goals for a player. \n",
    "    Args: df (player's gw DataFrame) ,week (int)\n",
    "    \"\"\"\n",
    "    gs = df['goals_scored'][0:week+1].value_counts().sort_index()\n",
    "    if gs.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0, 0, 0, 0]  # Some prior we can change later. This assume p(0 Goals) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(4+1))\n",
    "        stats = idx.join(gs).fillna(0)\n",
    "        return [stats.goals_scored[x]/stats.goals_scored.sum() for x in range(len(stats))]\n",
    "\n",
    "# ## Expected Assists\n",
    "def xAprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected assists for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    ass = df['assists'][0:week+1].value_counts().sort_index()\n",
    "    if ass.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0, 0, 0]  # Some prior we can change later. This assume p(0 assists) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(3+1))\n",
    "        stats = idx.join(ass).fillna(0)\n",
    "        return [stats.assists[x]/stats.assists.sum() for x in range(len(stats))]\n",
    "\n",
    "# ## Expected Bonus\n",
    "def xBprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected bonus for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    bon = df['bonus'][0:week+1].value_counts().sort_index()\n",
    "    if bon.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0, 0, 0]  # Some prior we can change later. This assume p(0 bonus) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(3+1))\n",
    "        stats = idx.join(bon).fillna(0)\n",
    "        return [stats.bonus[x]/stats.bonus.sum() for x in range(len(stats))]\n",
    "\n",
    "# ## Expected Clean Sheets\n",
    "\n",
    "def xCSprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected clean sheets for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    cs = df['clean_sheets'][0:week+1].value_counts().sort_index()\n",
    "    if cs.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(1+1))\n",
    "        stats = idx.join(cs).fillna(0)\n",
    "        return [stats.clean_sheets[x]/stats.clean_sheets.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "# ## Expected Goals Conceded\n",
    "def xGCprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected goals conceded for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    gc = df['goals_conceded'][0:week+1].value_counts().sort_index()\n",
    "    if gc.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0., 0., 0., 0., 0., 0., 0., 0., 0.]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(9+1))\n",
    "        stats = idx.join(gc).fillna(0)\n",
    "        return [stats.goals_conceded[x]/stats.goals_conceded.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "# ## Expected Minutes\n",
    "\n",
    "\n",
    "def xMprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected minutes for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    p = np.zeros(91)\n",
    "    p[0] = 1.0\n",
    "    M = df['minutes'][0:week+1].value_counts().sort_index()\n",
    "    if M.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return p  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(90+1))\n",
    "        stats = idx.join(M).fillna(0)\n",
    "        return [stats.minutes[x]/stats.minutes.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "# ## Expected Own Goals\n",
    "\n",
    "\n",
    "def xOGprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected own goals for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    og = df['own_goals'][0:week+1].value_counts().sort_index()\n",
    "    if og.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(1+1))\n",
    "        stats = idx.join(og).fillna(0)\n",
    "        return [stats.own_goals[x]/stats.own_goals.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "\n",
    "# ## Expected Penalties Conceded\n",
    "\n",
    "\n",
    "def xPCprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected penalties conceded for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pc = df['penalties_conceded'][0:week+1].value_counts().sort_index()\n",
    "    except KeyError: # this column is missing in 2019/2020 data for some players\n",
    "        return [1,0,0]\n",
    "    if pc.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0, 0]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(2+1))\n",
    "        stats = idx.join(pc).fillna(0)\n",
    "        return [stats.penalties_conceded[x]/stats.penalties_conceded.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "# ## Expected Penalties Missed\n",
    "\n",
    "def xPMprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected penalties missed for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    pm = df['penalties_missed'][0:week+1].value_counts().sort_index()\n",
    "    if pm.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(1+1))\n",
    "        stats = idx.join(pm).fillna(0)\n",
    "        return [stats.penalties_missed[x]/stats.penalties_missed.sum() for x in range(len(stats))]\n",
    "\n",
    "# ## Expected Penalties Saved\n",
    "\n",
    "def xPSprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected penalties saved for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    ps = df['penalties_saved'][0:week+1].value_counts().sort_index()\n",
    "    if ps.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0, 0]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(2+1))\n",
    "        stats = idx.join(ps).fillna(0)\n",
    "        return [stats.penalties_saved[x]/stats.penalties_saved.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Expected Red Cards\n",
    "\n",
    "def xRCprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected red cards for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    rc = df['red_cards'][0:week+1].value_counts().sort_index()\n",
    "    if rc.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(1+1))\n",
    "        stats = idx.join(rc).fillna(0)\n",
    "        return [stats.red_cards[x]/stats.red_cards.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "\n",
    "# ## Expected Saves\n",
    "\n",
    "def xSprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected saves for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    p = np.zeros(15)\n",
    "    p[0] = 1.0\n",
    "    s = df['saves'][0:week+1].value_counts().sort_index()\n",
    "    if s.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return p  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(14+1))\n",
    "        stats = idx.join(s).fillna(0)\n",
    "        return [stats.saves[x]/stats.saves.sum() for x in range(len(stats))]\n",
    "\n",
    "\n",
    "# ## Expected Yellow Cards\n",
    "\n",
    "\n",
    "def xYCprobs(df, week):\n",
    "    \"\"\"\n",
    "    Calculate distribution of expected yellow cards for a player. \n",
    "    Args: df (player's gw DataFrame), week (int)\n",
    "    \"\"\"\n",
    "    yc = df['yellow_cards'][0:week+1].value_counts().sort_index()\n",
    "    if yc.empty == True:  ## WHEN WE DON'T HAVE ANY DATA\n",
    "        return [1, 0]  # Some prior we can change later. This assume p(0 CS) = 1\n",
    "    else:\n",
    "        idx = pd.DataFrame(index=np.arange(1+1))\n",
    "        stats = idx.join(yc).fillna(0)\n",
    "        return [stats.yellow_cards[x]/stats.yellow_cards.sum() for x in range(len(stats))]\n",
    "        \n",
    "#generate realization\n",
    "#no need to use this for accessing historical data because it's already stored? \n",
    "#need to add penalties conceded\n",
    "def getPts(x, position):\n",
    "    \"\"\" \n",
    "    Use with getXpts for simulating reward.\n",
    "    \"\"\"\n",
    "    pts = 0\n",
    "    pts += x['bonus']\n",
    "    if ((x['minutes'] > 0) & (x['minutes'] < 60)):\n",
    "        pts += 1\n",
    "    elif (x['minutes'] >=60):\n",
    "        pts += 2\n",
    "    if position == 1.0:  # Goalie\n",
    "        pts += 6*x['goals_scored'] + x['assists']*3 + 4*x['clean_sheets'] + int(x['saves']//3) + 5*x['penalties_saved'] - x['goals_conceded']//2 - x['yellow_cards'] - 3*x['red_cards'] - 2*x['own_goals']   \n",
    "    elif position == 2.0:  # Defender\n",
    "        pts += 6*x['goals_scored'] + x['assists']*3 + 4*x['clean_sheets'] - 2*x['penalties_missed']  - x['goals_conceded']//2 - x['yellow_cards'] - 3*x['red_cards'] - 2*x['own_goals']\n",
    "    elif position == 3.0: # Mid\n",
    "        pts += 5*x['goals_scored'] + x['assists']*3 + 1*x['clean_sheets'] - 2*x['penalties_missed'] - x['yellow_cards'] - 3*x['red_cards'] - 2*x['own_goals']\n",
    "    else:  # Striker\n",
    "        pts += 4*x['goals_scored'] + x['assists']*3 - 2*x['penalties_missed'] - x['yellow_cards'] - 3*x['red_cards'] - 2*x['own_goals']\n",
    "    return pts\n",
    "    \n",
    "def ilocfromloc(player):\n",
    "    \"\"\"\n",
    "    Use this function to get the relative position of a player in the full_names series.\n",
    "    Helpful because other objects such as xPs use the same ordering as full_names.\n",
    "    \"\"\"\n",
    "    return full_names.index.get_loc(full_names[full_names==player].index[0])\n",
    "\n",
    "def getPlayerPool(team=curTeam):\n",
    "    player_pool = set(full_names) - set(team.full_name)\n",
    "    pp = player_initial_data.loc[player_initial_data[\"full_names\"].isin(player_pool)][[\"full_names\", \"element_type\", 'team']]\n",
    "    pp['xP'] = 0\n",
    "    return pp\n",
    "\n",
    "def updateXP(week,team=curTeam):\n",
    "    for player in team.full_name.values:\n",
    "        team.loc[team.full_name == player, 'xP'] = getXpts(get_df(player), week, team.loc[team.full_name == player]['position'].values[0])\n",
    "    return team\n",
    "\n",
    "def selectCaptain(team=curTeam):\n",
    "    \"\"\"\n",
    "    input your starting squad and this function picks the captain based on whose xP is highest\n",
    "    \"\"\"\n",
    "    getCaptain = [1 if team['xP'].iloc[x] == team.xP.max() else 0 for x in range(len(team))]\n",
    "    team['isCaptain'] = getCaptain\n",
    "    return team\n",
    "\n",
    "def getTopN(df, position, week,N, max_budget):\n",
    "    for player in df.full_names.loc[df.element_type==position]: \n",
    "        #print(player)\n",
    "        df.loc[df.full_names == player, \"xP\"] = getXpts(get_df(player),week,position)\n",
    "        #df.loc[df.full_names == player, \"value\"] = get_df(player)['value'][week]\n",
    "        try:\n",
    "            df.loc[df.full_names == player, \"value\"] = get_df(player).loc[get_df(player)['round'] == week]['value'].iloc[0]\n",
    "        except IndexError: # If this player wasn't available in the week, we'll be able to exclude them using the following\n",
    "            df.loc[df.full_names == player, \"value\"] = max_budget + 1 \n",
    "    temp = df.loc[df.element_type==position].sort_values(by='xP',ascending=False)\n",
    "    temp = temp[temp.value <= max_budget]\n",
    "    return temp[0:N].rename(columns={\"full_names\":'full_name', \"element_type\":\"position\"})\n",
    "\n",
    "def worstAtPos(pos, team=curTeam, bud=budget):\n",
    "    \"\"\"\n",
    "    Input your current team, the current budget, and the position to replace.\n",
    "    Returns the player who has the lowest expected points per value at the specified position and the available budget if\n",
    "    that player were replaced\n",
    "    \"\"\"\n",
    "    options = team.loc[team.position == pos] # players \n",
    "    worst = options.iloc[np.argmin([options.iloc[x]['xP']/options.iloc[x]['value'] for x in range(len(options))])]\n",
    "    return worst.full_name, bud+worst.value\n",
    "\n",
    "def swapPlayer(team, playerOut, choices, idx, curBudget):\n",
    "    \"\"\"\n",
    "    Given players to swap and the current budget\n",
    "    Swaps a the player from the current team with the player in the player pool and updates the budget\n",
    "    Should figure out a way to integrate this with epsilon-greedy policy\n",
    "    \"\"\"\n",
    "    try:\n",
    "        newTeam = pd.concat([team.loc[team.full_name!=playerOut], choices.iloc[[idx]]])\n",
    "    except IndexError:  # Occurs when there's no one to swap\n",
    "        print(\"Cannot Swap Player this round using this action.\")\n",
    "        return team, curBudget\n",
    "    newBudget = curBudget + team.loc[team.full_name==playerOut]['value'].values[0] - choices.iloc[[idx]]['value'].values[0]\n",
    "    return newTeam, newBudget   \n",
    "\n",
    "def updateSquad(action, week, team=curTeam, bud=budget, PAs=3):\n",
    "    if action == 0: # Do Nothing\n",
    "        return team, bud\n",
    "    elif action < PAs+1: # Replace GoalKeeper\n",
    "        worstPlayer, potential_budget = worstAtPos(pos=1, team=team, bud=bud)\n",
    "        topN = getTopN(df=pp, position=1, week=week, N=PAs, max_budget=potential_budget)\n",
    "        team, budget = swapPlayer(team=team, playerOut=worstPlayer, choices=topN, idx=action-1, curBudget=bud)\n",
    "        return team, budget\n",
    "    elif action < PAs+4: # Replace Defender\n",
    "        worstPlayer, potential_budget = worstAtPos(pos=2, team=team, bud=bud)\n",
    "        topN = getTopN(df=pp, position=2, week=week, N=PAs, max_budget=potential_budget)\n",
    "        team, budget = swapPlayer(team=team, playerOut=worstPlayer, choices=topN, idx=action-PAs-1, curBudget=bud)\n",
    "        return team, budget\n",
    "    elif action < PAs+7: # Replace Midfielder\n",
    "        worstPlayer, potential_budget = worstAtPos(pos=3, team=team, bud=bud)\n",
    "        topN = getTopN(df=pp, position=3, week=week, N=PAs, max_budget=potential_budget)\n",
    "        team, budget = swapPlayer(team=team, playerOut=worstPlayer, choices=topN, idx=action-2*PAs-1, curBudget=bud)\n",
    "        return team, budget\n",
    "    else: # Replace Forward\n",
    "        worstPlayer, potential_budget = worstAtPos(pos=4, team=team, bud=bud)\n",
    "        topN = getTopN(df=pp, position=4, week=week, N=PAs, max_budget=potential_budget)\n",
    "        team, budget = swapPlayer(team=team, playerOut=worstPlayer, choices=topN, idx=action-3*PAs-1, curBudget=bud)\n",
    "        return team, budget\n",
    "    return\n",
    "\n",
    "def get_weekly_pts_data(week):\n",
    "    pts = []\n",
    "    for player in list(curTeam.loc[curTeam.isStarting == 1].full_name):\n",
    "        df = get_df(player)\n",
    "        idx = df['round'] == week\n",
    "        try:\n",
    "            pt = df['total_points'][idx].iloc[0]\n",
    "        except IndexError:\n",
    "            pt = 2  # If our starter doesn't play this week, assume their replacement got 2 pts (played 60+ min)\n",
    "        if curTeam.loc[curTeam['full_name'] == player, 'isCaptain'].values[0] == 1:\n",
    "            pt *=2\n",
    "        pts.append(pt)\n",
    "    return sum(pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use our Q function trained on 100 episodes of the 2018-2019 season using SARSA and a $\\gamma$ of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "########## SARSA ##########\n",
    "###########################\n",
    "# Load in trained Q Function\n",
    "with open('SARSA1.pkl', 'rb') as f:  \n",
    "    Qsarsa, _, _ = pickle.load(f)\n",
    "# Initialize Algorithm Parameters\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "gamma = 1\n",
    "# gamma = 0.5 # Suggested by Mathews \n",
    "ssize = 2000 # number of possible states ==> for each possible budget and team configuration\n",
    "asize = 13 # number of possible actions in each state: 13 now because do nothing + 3 PAs for each of 4 positions\n",
    "numEpisodes = 100\n",
    "weeks = list(range(2,39))\n",
    "sarsaRewards = np.zeros((numEpisodes,))\n",
    "WEEKLY_SCORES = np.zeros((numEpisodes,1,38+1))\n",
    "# Algorithm Functions\n",
    "def get_action(state, Q, epsilon):\n",
    "    Q_s = Q[state,:] # Get state-action rewards corresponding to the current state\n",
    "    if (np.random.uniform() <= epsilon):\n",
    "        action = np.random.choice(a=list(range(asize))) # Chose action randomily with pr 1-epsilon\n",
    "    else: # Choose greedy action with pr 1-epsilon\n",
    "        action = np.random.choice(np.flatnonzero(Q_s == Q_s.max())) \n",
    "    return action\n",
    "\n",
    "def SARSAupdate_Q(Q, S, Sprime, A, Aprime, alpha, R, gamma):\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Q[Sprime,Aprime] - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "def QLupdateQ(Q, S, Sprime, A, alpha, R, gamma):\n",
    "    Qmax = np.max(Q[Sprime, :])\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Qmax - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "ST = time.time()\n",
    "\n",
    "for i in range(numEpisodes):\n",
    "    episodeST = time.time()\n",
    "    print(\"starting episode {}\".format(i))\n",
    "    curTeam, budget = initializeTeam() # initialize the team and budget (state)\n",
    "    current_state = budget  # initialize S\n",
    "    sarsaRewards[i] += get_weekly_pts_data(1)\n",
    "    WEEKLY_SCORES[i,0,0] += get_weekly_pts_data(1)\n",
    "    for curWeek in weeks: # For each step in the episode\n",
    "        current_action = get_action(state=current_state, Q=Qsarsa, epsilon=epsilon) # Choose A from S using policy derived from Q\n",
    "        pp = getPlayerPool(team=curTeam) # Start by getting the available player pool\n",
    "        curTeam = updateXP(week=curWeek) # Get expectations for your current team's points\n",
    "        curTeam, next_state = updateSquad(current_action, week=curWeek, team=curTeam, bud=current_state)  # Update squad based on action, transition to new state (budget)\n",
    "        curTeam['isStarting'] = pick_starters(curTeam, init=False) # Choose your starters once you've updated the squad\n",
    "        curTeam = selectCaptain()  # Choose the captain once you have the starters\n",
    "        sarsaRewards[i] += get_weekly_pts_data(curWeek) # Take Action a, observe\n",
    "        WEEKLY_SCORES[i,0,curWeek] += get_weekly_pts_data(curWeek) # Realize GW results, update your Rewards\n",
    "        next_action = get_action(int(next_state), Qsarsa, epsilon)  # Choose A' from S' using policy derived from Q\n",
    "        #Qsarsa = SARSAupdate_Q(Q=Qsarsa, S=int(current_state), Sprime=int(next_state), A=int(current_action), Aprime=int(next_action), alpha=alpha, R=WEEKLY_SCORES[i,0,curWeek], gamma=gamma)\n",
    "        current_state = int(next_state) # S <- S'\n",
    "        current_action = int(next_action) # A <- A'\n",
    "    episodeEND = time.time()\n",
    "    print(\"Episode {} took {:.2f} minutes to run. EPISODE PTS = {}\".format(i, (episodeEND-episodeST)/60, sarsaRewards[i]))\n",
    "    \n",
    "END = time.time()\n",
    "\n",
    "print(END-ST)\n",
    "\n",
    "with open('SARSA1_1920_noUpdate.pkl', 'wb') as f:  \n",
    "    pickle.dump([Qsarsa, sarsaRewards, WEEKLY_SCORES], f)  # SAVE THE Tested Q function, Episodic Rewards, and Weekly Rewards\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(sarsaRewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Season Rewards\")\n",
    "plt.title(\"SARSA, γ=1.0\")\n",
    "plt.savefig(\"SARSA_Testing1920_gamma1_NoUpdate.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(sarsaRewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Season Rewards\")\n",
    "plt.title(\"SARSA, γ=1.0\")\n",
    "plt.savefig(\"SARSA_Testing1920_gamma1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use our Q function trained on 100 episodes of the 2018-2019 season using SARSA and a $\\gamma$ of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "########## SARSA ##########\n",
    "###########################\n",
    "# Load in trained Q Function\n",
    "with open('SARSA5.pkl', 'rb') as f:  \n",
    "    Qsarsa, _, _ = pickle.load(f)\n",
    "# Initialize Algorithm Parameters\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "gamma = 0.5 # Suggested by Mathews \n",
    "ssize = 2000 # number of possible states ==> for each possible budget and team configuration\n",
    "asize = 13 # number of possible actions in each state: 13 now because do nothing + 3 PAs for each of 4 positions\n",
    "numEpisodes = 100\n",
    "weeks = list(range(2,39))\n",
    "sarsaRewards = np.zeros((numEpisodes,))\n",
    "WEEKLY_SCORES = np.zeros((numEpisodes,1,38+1))\n",
    "# Algorithm Functions\n",
    "def get_action(state, Q, epsilon):\n",
    "    Q_s = Q[state,:] # Get state-action rewards corresponding to the current state\n",
    "    if (np.random.uniform() <= epsilon):\n",
    "        action = np.random.choice(a=list(range(asize))) # Chose action randomily with pr 1-epsilon\n",
    "    else: # Choose greedy action with pr 1-epsilon\n",
    "        action = np.random.choice(np.flatnonzero(Q_s == Q_s.max())) \n",
    "    return action\n",
    "\n",
    "def SARSAupdate_Q(Q, S, Sprime, A, Aprime, alpha, R, gamma):\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Q[Sprime,Aprime] - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "def QLupdateQ(Q, S, Sprime, A, alpha, R, gamma):\n",
    "    Qmax = np.max(Q[Sprime, :])\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Qmax - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "ST = time.time()\n",
    "\n",
    "for i in range(numEpisodes):\n",
    "    episodeST = time.time()\n",
    "    print(\"starting episode {}\".format(i))\n",
    "    curTeam, budget = initializeTeam() # initialize the team and budget (state)\n",
    "    current_state = budget  # initialize S\n",
    "    sarsaRewards[i] += get_weekly_pts_data(1)\n",
    "    WEEKLY_SCORES[i,0,0] += get_weekly_pts_data(1)\n",
    "    for curWeek in weeks: # For each step in the episode\n",
    "        current_action = get_action(state=current_state, Q=Qsarsa, epsilon=epsilon) # Choose A from S using policy derived from Q\n",
    "        pp = getPlayerPool(team=curTeam) # Start by getting the available player pool\n",
    "        curTeam = updateXP(week=curWeek) # Get expectations for your current team's points\n",
    "        curTeam, next_state = updateSquad(current_action, week=curWeek, team=curTeam, bud=current_state)  # Update squad based on action, transition to new state (budget)\n",
    "        curTeam['isStarting'] = pick_starters(curTeam, init=False) # Choose your starters once you've updated the squad\n",
    "        curTeam = selectCaptain()  # Choose the captain once you have the starters\n",
    "        sarsaRewards[i] += get_weekly_pts_data(curWeek) # Take Action a, observe\n",
    "        WEEKLY_SCORES[i,0,curWeek] += get_weekly_pts_data(curWeek) # Realize GW results, update your Rewards\n",
    "        next_action = get_action(int(next_state), Qsarsa, epsilon)  # Choose A' from S' using policy derived from Q\n",
    "        Qsarsa = SARSAupdate_Q(Q=Qsarsa, S=int(current_state), Sprime=int(next_state), A=int(current_action), Aprime=int(next_action), alpha=alpha, R=WEEKLY_SCORES[i,0,curWeek], gamma=gamma)\n",
    "        current_state = int(next_state) # S <- S'\n",
    "        current_action = int(next_action) # A <- A'\n",
    "    episodeEND = time.time()\n",
    "    print(\"Episode {} took {:.2f} minutes to run. EPISODE PTS = {}\".format(i, (episodeEND-episodeST)/60, sarsaRewards[i]))\n",
    "    \n",
    "END = time.time()\n",
    "\n",
    "print(END-ST)\n",
    "\n",
    "with open('SARSA5_1920.pkl', 'wb') as f:  \n",
    "    pickle.dump([Qsarsa, sarsaRewards, WEEKLY_SCORES], f)  # SAVE THE Tested Q function, Episodic Rewards, and Weekly Rewards\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(sarsaRewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Season Rewards\")\n",
    "plt.title(\"SARSA, γ=1.0\")\n",
    "plt.savefig(\"SARSA_Testing1920_gamma5.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use our Q function trained on 100 episodes of the 2018-2019 season using Q-Learning and a $\\gamma$ of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "########## Q-Learning ##########\n",
    "################################\n",
    "# Load in trained Q Function\n",
    "with open('QL1.pkl', 'rb') as f:  \n",
    "    Qlearning, _, _ = pickle.load(f)\n",
    "# Initialize Algorithm Parameters\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "gamma = 1\n",
    "#gamma = 0.5 # Suggested by Mathews \n",
    "ssize = 2000 # number of possible states ==> set to something sufficiently large so that we don't get IndexError\n",
    "asize = 13 # number of possible actions in each state: 13 now because do nothing + 3 PAs for each of 4 positions\n",
    "numEpisodes = 100\n",
    "weeks = list(range(2,39))\n",
    "QLRewards = np.zeros((numEpisodes,))\n",
    "QL_WEEKLY_SCORES = np.zeros((numEpisodes,1,38+1))\n",
    "# Algorithm Functions\n",
    "def get_action(state, Q, epsilon):\n",
    "    Q_s = Q[state,:] # Get state-action rewards corresponding to the current state\n",
    "    if (np.random.uniform() <= epsilon):\n",
    "        action = np.random.choice(a=list(range(asize))) # Chose action randomily with pr 1-epsilon\n",
    "    else: # Choose greedy action with pr 1-epsilon\n",
    "        action = np.random.choice(np.flatnonzero(Q_s == Q_s.max())) \n",
    "    return action\n",
    "\n",
    "def SARSAupdate_Q(Q, S, Sprime, A, Aprime, alpha, R, gamma):\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Q[Sprime,Aprime] - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "def QLupdateQ(Q, S, Sprime, A, alpha, R, gamma):\n",
    "    Qmax = np.max(Q[Sprime, :])\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Qmax - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "ST = time.time()\n",
    "\n",
    "for i in range(numEpisodes):\n",
    "    episodeST = time.time()\n",
    "    print(\"starting episode {}\".format(i))\n",
    "    curTeam, budget = initializeTeam() # initialize the team and budget (state)\n",
    "    current_state = budget  # initialize S\n",
    "    QLRewards[i] += get_weekly_pts_data(1)\n",
    "    QL_WEEKLY_SCORES[i,0,0] += get_weekly_pts_data(1)\n",
    "    for curWeek in weeks: # For each step in the episode\n",
    "        current_action = get_action(state=current_state, Q=Qlearning, epsilon=epsilon) # Choose A from S using policy derived from Q\n",
    "        pp = getPlayerPool(team=curTeam) # Start by getting the available player pool\n",
    "        curTeam = updateXP(week=curWeek) # Get expectations for your current team's points\n",
    "        curTeam, next_state = updateSquad(current_action, week=curWeek, team=curTeam, bud=current_state)  # Update squad based on action, transition to new state (budget)\n",
    "        curTeam['isStarting'] = pick_starters(curTeam, init=False) # Choose your starters once you've updated the squad\n",
    "        curTeam = selectCaptain()  # Choose the captain once you have the starters\n",
    "        QLRewards[i] += get_weekly_pts_data(curWeek) # Take Action a, observe\n",
    "        QL_WEEKLY_SCORES[i,0,curWeek] += get_weekly_pts_data(curWeek) # Realize GW results, update your Rewards\n",
    "        Qlearning = QLupdateQ(Q=Qlearning, S=int(current_state), Sprime=int(next_state), A=int(current_action), alpha=alpha, R=QL_WEEKLY_SCORES[i,0,curWeek], gamma=gamma)\n",
    "        current_state = int(next_state) # S <- S'\n",
    "    episodeEND = time.time()\n",
    "    print(\"Episode {} took {:.2f} minutes to run. EPISODE PTS = {}\".format(i, (episodeEND-episodeST)/60, QLRewards[i]))\n",
    "    \n",
    "END = time.time()\n",
    "\n",
    "print(END-ST)\n",
    "\n",
    "with open('QL1_1920.pkl', 'wb') as f:  \n",
    "    pickle.dump([Qlearning, QLRewards, QL_WEEKLY_SCORES], f)  # SAVE THE Tested Q function, Episodic Rewards, and Weekly Rewards\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(QLRewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Season Rewards\")\n",
    "plt.title(\"Q-Learning, γ=1.0\")\n",
    "plt.savefig(\"QL_Testing1920_gamma1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use our Q function trained on 100 episodes of the 2018-2019 season using Q-Learning and a $\\gamma$ of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "########## Q-Learning ##########\n",
    "################################\n",
    "# Load in trained Q Function\n",
    "with open('QL5.pkl', 'rb') as f:  \n",
    "    Qlearning, _, _ = pickle.load(f)\n",
    "# Initialize Algorithm Parameters\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "# gamma = 1\n",
    "gamma = 0.5 # Suggested by Mathews \n",
    "ssize = 2000 # number of possible states ==> set to something sufficiently large so that we don't get IndexError\n",
    "asize = 13 # number of possible actions in each state: 13 now because do nothing + 3 PAs for each of 4 positions\n",
    "numEpisodes = 100\n",
    "weeks = list(range(2,39))\n",
    "QLRewards = np.zeros((numEpisodes,))\n",
    "QL_WEEKLY_SCORES = np.zeros((numEpisodes,1,38+1))\n",
    "# Algorithm Functions\n",
    "def get_action(state, Q, epsilon):\n",
    "    Q_s = Q[state,:] # Get state-action rewards corresponding to the current state\n",
    "    if (np.random.uniform() <= epsilon):\n",
    "        action = np.random.choice(a=list(range(asize))) # Chose action randomily with pr 1-epsilon\n",
    "    else: # Choose greedy action with pr 1-epsilon\n",
    "        action = np.random.choice(np.flatnonzero(Q_s == Q_s.max())) \n",
    "    return action\n",
    "\n",
    "def SARSAupdate_Q(Q, S, Sprime, A, Aprime, alpha, R, gamma):\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Q[Sprime,Aprime] - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "def QLupdateQ(Q, S, Sprime, A, alpha, R, gamma):\n",
    "    Qmax = np.max(Q[Sprime, :])\n",
    "    Q[S,A] = Q[S,A] + alpha*(R+gamma*Qmax - Q[S,A])\n",
    "    return Q\n",
    "\n",
    "ST = time.time()\n",
    "\n",
    "for i in range(numEpisodes):\n",
    "    episodeST = time.time()\n",
    "    print(\"starting episode {}\".format(i))\n",
    "    curTeam, budget = initializeTeam() # initialize the team and budget (state)\n",
    "    current_state = budget  # initialize S\n",
    "    QLRewards[i] += get_weekly_pts_data(1)\n",
    "    QL_WEEKLY_SCORES[i,0,0] += get_weekly_pts_data(1)\n",
    "    for curWeek in weeks: # For each step in the episode\n",
    "        current_action = get_action(state=current_state, Q=Qlearning, epsilon=epsilon) # Choose A from S using policy derived from Q\n",
    "        pp = getPlayerPool(team=curTeam) # Start by getting the available player pool\n",
    "        curTeam = updateXP(week=curWeek) # Get expectations for your current team's points\n",
    "        curTeam, next_state = updateSquad(current_action, week=curWeek, team=curTeam, bud=current_state)  # Update squad based on action, transition to new state (budget)\n",
    "        curTeam['isStarting'] = pick_starters(curTeam, init=False) # Choose your starters once you've updated the squad\n",
    "        curTeam = selectCaptain()  # Choose the captain once you have the starters\n",
    "        QLRewards[i] += get_weekly_pts_data(curWeek) # Take Action a, observe\n",
    "        QL_WEEKLY_SCORES[i,0,curWeek] += get_weekly_pts_data(curWeek) # Realize GW results, update your Rewards\n",
    "        Qlearning = QLupdateQ(Q=Qlearning, S=int(current_state), Sprime=int(next_state), A=int(current_action), alpha=alpha, R=QL_WEEKLY_SCORES[i,0,curWeek], gamma=gamma)\n",
    "        current_state = int(next_state) # S <- S'\n",
    "    episodeEND = time.time()\n",
    "    print(\"Episode {} took {:.2f} minutes to run. EPISODE PTS = {}\".format(i, (episodeEND-episodeST)/60, QLRewards[i]))\n",
    "    \n",
    "END = time.time()\n",
    "\n",
    "print(END-ST)\n",
    "\n",
    "with open('QL5_1920.pkl', 'wb') as f:  \n",
    "    pickle.dump([Qlearning, QLRewards, QL_WEEKLY_SCORES], f)  # SAVE THE Tested Q function, Episodic Rewards, and Weekly Rewards\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(QLRewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Season Rewards\")\n",
    "plt.title(\"Q-Learning, γ=0.5\")\n",
    "plt.savefig(\"QL_Testing1920_gamma5.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose actions randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSims = 100\n",
    "action = np.random.choice(a=list(range(13)))\n",
    "weeks = list(range(2,39))\n",
    "# weeks = list(range(2,27))\n",
    "startTime = time.time()\n",
    "CURRENT_SCORE = np.zeros((numSims,))\n",
    "WEEKLY_SCORE = []\n",
    "curTeam, budget = initializeTeam()\n",
    "WEEKLY_SCORE.append(get_weekly_pts_data(1))\n",
    "CURRENT_SCORE += get_weekly_pts_data(1)\n",
    "action_list = []  # Keep track of actions\n",
    "for i in range(numSims):\n",
    "    print(\"starting simulation {}\".format(i))\n",
    "    for curWeek in weeks:\n",
    "        curTeam, budget = initializeTeam()\n",
    "        action = np.random.choice(a=list(range(13)))\n",
    "        action_list.append(action)\n",
    "        pp = getPlayerPool(team=curTeam) # Start by getting the available player pool\n",
    "        curTeam = updateXP(week=curWeek) # Get expectations for your current team's points\n",
    "        curTeam, budget = updateSquad(action, week=curWeek, team=curTeam, bud=budget)  # Update squad based on action, state (budget)\n",
    "        curTeam['isStarting'] = pick_starters(curTeam, init=False) # Choose your starters once you've updated the squad\n",
    "        curTeam = selectCaptain()  # Choose the captain once you have the starters\n",
    "        WEEKLY_SCORE.append(get_weekly_pts_data(curWeek)) # Realize GW results, update your Rewards\n",
    "        CURRENT_SCORE[i] += get_weekly_pts_data(curWeek)  # Realize GW results, update your Rewards\n",
    "endTime = time.time()\n",
    "print(\"{:.2f} minutes\".format((endTime-startTime)/60))\n",
    "print(\"End of Season Score: {}\".format(CURRENT_SCORE[i]))\n",
    "\n",
    "with open('RandomAction1920.pkl', 'wb') as f:  \n",
    "    pickle.dump([CURRENT_SCORE, WEEKLY_SCORE], f)  # SAVE THE Trained Q function, Episodic Rewards, and Weekly Rewards\n",
    "\n",
    "with open('RandomAction1920.pkl', 'rb') as f:  \n",
    "    obj0, obj1 = pickle.load(f)\n",
    "\n",
    "plt.plot(CURRENT_SCORE)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Season Rewards\")\n",
    "plt.title(\"Random Action Selection: 2019-2020 Season\")\n",
    "plt.savefig(\"RandomActionSelection100Sims.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"QL5_1920_noUpdate.pkl\", 'rb') as f: # Q-Learning, gamma=0.5\n",
    "    _, QL5, _ = pickle.load(f)\n",
    "    \n",
    "with open(\"QL1_1920_noUpdate.pkl\", 'rb') as f: # Q-Learning, gamma=1\n",
    "    _, QL1, _ = pickle.load(f)\n",
    "\n",
    "with open(\"SARSA1_1920_noUpdate.pkl\", 'rb') as f: # SARSA, gamma=1\n",
    "    _, sarsa1, _ = pickle.load(f)\n",
    "    \n",
    "with open(\"SARSA5_1920_noUpdate.pkl\", 'rb') as f: # SARSA, gamma=0.5\n",
    "    _, sarsa5, _ = pickle.load(f)\n",
    "\n",
    "with open(\"RandomAction1920.pkl\", 'rb') as f:  # Taking random actions\n",
    "    randomAction, _ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(sarsa1)\n",
    "plt.hist(sarsa5)\n",
    "plt.hist(QL1)\n",
    "plt.hist(QL5)\n",
    "plt.hist(randomAction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Descriptive Statistics\n",
    "def get_stats(alg, gamma, data):\n",
    "    print(\"Algorithm: {}\".format(alg))\n",
    "    print(\"γ: {}\".format(gamma))\n",
    "    print(\"Max: {:.0f}\".format(data.max()))\n",
    "    print(\"Minimum: {:.0f}\".format(data.min()))\n",
    "    print(\"Mean: {:.0f}\".format(data.mean()))\n",
    "    print(\"Variance: {:.0f}\".format(np.var(data)))\n",
    "    print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats(\"SARSA\", 1.0, sarsa1)\n",
    "get_stats(\"SARSA\", 0.5, sarsa5)\n",
    "get_stats(\"Q-Learning\", 1.0, QL1)\n",
    "get_stats(\"Q-Learning\", 0.5, QL5)\n",
    "get_stats(\"Random Actions\", 'N/A', randomAction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.plot(QL5, label=\"Q-Learning, γ=0.5\",linewidth=2)\n",
    "plt.plot(QL1, label=\"Q-Learning, γ=1\",linewidth=2)\n",
    "plt.plot(sarsa5, label=\"SARSA, γ=0.5\",linewidth=2)\n",
    "plt.plot(sarsa1, label=\"SARSA, γ=0.5\",linewidth=2)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "#plt.plot(pd.DataFrame(QL5).rolling(10).mean(), label=\"Q-Learning, γ=0.5\",linewidth=1.5)\n",
    "#plt.plot(pd.DataFrame(QL1).rolling(10).mean(), label=\"Q-Learning, γ=1\",linewidth=1.5)\n",
    "#plt.plot(pd.DataFrame(sarsa5).rolling(10).mean(), label=\"SARSA, γ=0.5\",linewidth=1.5)\n",
    "#plt.plot(pd.DataFrame(sarsa1).rolling(10).mean(), label=\"SARSA, γ=0.5\",linewidth=1.5)\n",
    "plt.plot(pd.DataFrame(randomAction).rolling(10).mean(), label=\"Random Action\",linewidth=1.5)\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"QL5.pkl\", 'rb') as f: # Q-Learning, gamma=0.5\n",
    "    _, QL5, _ = pickle.load(f)\n",
    "    \n",
    "with open(\"QL1.pkl\", 'rb') as f: # Q-Learning, gamma=1\n",
    "    _, QL1, _ = pickle.load(f)\n",
    "\n",
    "with open(\"SARSA1.pkl\", 'rb') as f: # SARSA, gamma=1\n",
    "    _, sarsa1, _ = pickle.load(f)\n",
    "    \n",
    "with open(\"SARSA5.pkl\", 'rb') as f: # SARSA, gamma=0.5\n",
    "    _, sarsa5, _ = pickle.load(f)\n",
    "\n",
    "with open(\"RandomAction.pkl\", 'rb') as f:  # Taking random actions\n",
    "    randomAction, _ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats(\"SARSA\", 1.0, sarsa1)\n",
    "get_stats(\"SARSA\", 0.5, sarsa5)\n",
    "get_stats(\"Q-Learning\", 1.0, QL1)\n",
    "get_stats(\"Q-Learning\", 0.5, QL5)\n",
    "get_stats(\"Random Actions\", 'N/A', randomAction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "#plt.figure()\n",
    "#plt.ylim((1900,1945))\n",
    "plt.tight_layout()\n",
    "plt.gcf().subplots_adjust(bottom=0.15, left=0.15)\n",
    "plt.ticklabel_format(useOffset=False) #useMathText=False,style='plain'\n",
    "# plt.plot(pd.DataFrame(QL5).rolling(10).mean(), label=\"Q-Learning, γ=0.5\",linewidth=2)\n",
    "# plt.plot(pd.DataFrame(QL1).rolling(10).mean(), label=\"Q-Learning, γ=1\",linewidth=2)\n",
    "plt.plot(pd.DataFrame(sarsa1).rolling(10).mean(), label=\"SARSA, γ=1.0\",linewidth=2)\n",
    "plt.plot(pd.DataFrame(sarsa5).rolling(10).mean(), label=\"SARSA, γ=0.5\",linewidth=2)\n",
    "plt.plot(pd.DataFrame(randomAction).rolling(10).mean(), label=\"Random Action\",linewidth=2, color='black')\n",
    "#plt.title(\"Comparison of Algorithm Performance during Training\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend(loc='best')\n",
    "#plt.savefig(\"FORPAPER_SARSATestingPerformanceWithRandomAction.png\")\n",
    "#plt.savefig(\"FORPAPER_QLearningTestingPerformance.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
